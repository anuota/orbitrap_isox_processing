{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## script for preprocessing data from orbitrap\n",
    "<!--    # 1) splits the file based on times provided\n",
    "#    # 2) deletes unpaired compounds for each scan\n",
    "# (c) by Anya for Ilya 22.June 2023 with edits by Denis\n",
    "\n",
    "# run by typing in the command line inside the folder with files\n",
    "\n",
    "\n",
    "\n",
    "#    # python3 preproc_del_unpaired_isox.py \n",
    "# or in python notebook using \n",
    "#    # %run preproc_del_unpaired_isox.py \n",
    "\n",
    "# script reads all .isox files in the current folder as tables\n",
    "# cuts tables into 4 separate ones\n",
    "# resets time and scan number for each of the new tables\n",
    "# writes down the files in the folder named by the original file\n",
    "\n",
    "# if nesessary:\n",
    "# adjust cutting times in the def main:\n",
    "    # df1_split_times\n",
    "    # df2_split_times\n",
    "    # df3_split_times\n",
    "    # df4_split_times  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_isox(path, df1_split_times,df2_split_times,df3_split_times,df4_split_times):\n",
    "\n",
    "    #read in the file from the specified path\n",
    "    data_isox = pd.read_table(path, sep='\\s+')\n",
    "    \n",
    "    #print info\n",
    "    print('The total length of the file is ' + str(data_isox['time.min'].min())+' min to '+\n",
    "      str(data_isox['time.min'].max()) + ' min')\n",
    "    print('____________________________________')\n",
    "    print('The chosed split of the file is into 4 files \\n 1st ' \n",
    "        + str(df1_split_times) +\n",
    "        ' min \\n 2nd ' + str(df2_split_times) +\n",
    "        ' min \\n 3rd ' + str(df3_split_times) +\n",
    "        ' min \\n 4th ' + str(df4_split_times) +' min')\n",
    "\n",
    "    #split the dataframe into 4 based on provided data\n",
    "    df1 = data_isox[(data_isox['time.min'] > df1_split_times[0]) & (data_isox['time.min'] < df1_split_times[1])] \n",
    "    df2 = data_isox[(data_isox['time.min'] > df2_split_times[0]) & (data_isox['time.min'] < df2_split_times[1])]\n",
    "    df3 = data_isox[(data_isox['time.min'] > df3_split_times[0]) & (data_isox['time.min'] < df3_split_times[1])]\n",
    "    df4 = data_isox[(data_isox['time.min'] > df4_split_times[0]) & (data_isox['time.min'] < df4_split_times[1])]\n",
    "    \n",
    "    # reset times and scan number in the new dataframes\n",
    "    dfs_list=[df1,df2,df3,df4]\n",
    "    for df in dfs_list:\n",
    "        df.loc[:,'time.min']=df['time.min']-df['time.min'].min()+0.01 \n",
    "        df.loc[:,'scan.no']=df['scan.no']-df['scan.no'].min()+1\n",
    "    \n",
    "    return dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent(part, whole):\n",
    "    try:\n",
    "        return 100 * float(part) / float(whole)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prob_comp(dfs_list):\n",
    "    #a function to find a problematic compound\n",
    "    #dfs_list=[df1,df2,df3,df4]\n",
    "    i=1\n",
    "    for df in dfs_list:\n",
    "        print('___Working on file #'+str(i) )\n",
    "        i=i+1\n",
    "        #find problematic compunds\n",
    "        compounds=df['compound'].unique()\n",
    "        #print(compounds)\n",
    "        #print compound if its quantity does not match to number of scans\n",
    "        prob_comps=[comp for comp in compounds if df[(df['compound']==comp)]['isotopolog'].count()/2 != df['scan.no'].max()]\n",
    "        #print(prob_comps)\n",
    "        \n",
    "        # for each problmatic compound print number of scans and number of compounds   \n",
    "        for prob_co in prob_comps:\n",
    "            numm_scans=(df['scan.no'].max())\n",
    "            num_comp=df[df['compound']==prob_co]['isotopolog'].count()/2\n",
    "            #calculate percent of missing data\n",
    "            perc = 100 - percent(num_comp,numm_scans)\n",
    "            print('Problematic compound: '+str(prob_co) +'; No of scans/compounds: ' +str(numm_scans)+' / '  +str(num_comp) )\n",
    "            print(f'   missing {perc:.2f} % of the data' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unpaired(dfs_list):\n",
    "    # delete scans of compounds with unpaired isotopologs\n",
    "    print('Starting to delete unpaired lines')\n",
    "    dfs_list_new = []\n",
    "    for df in dfs_list:\n",
    "        # group by scan number\n",
    "        pairs = df.groupby(['scan.no', 'compound'])['isotopolog'].size().reset_index(name='counts')\n",
    "\n",
    "        # data to keep\n",
    "        pairs_to_keep = pairs[pairs['counts'] == 2].drop(columns='counts')\n",
    "\n",
    "        # keep only nesessary data in dataframe\n",
    "        df = df.merge(pairs_to_keep[['scan.no', 'compound']], on=['scan.no', 'compound'], how='inner')\n",
    "\n",
    "        dfs_list_new.append(df)\n",
    "        print('Unpaired lines are deleted')\n",
    "    return dfs_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(dfs_list):\n",
    "    # #write down the dataframes\n",
    "\n",
    "    print('____________________________________')\n",
    "    print('Currently saving:')\n",
    "    \n",
    "    for df in dfs_list:\n",
    "        outpath=df['filename'].iloc[0]\n",
    "        os.makedirs(outpath, exist_ok=True)  \n",
    "        name=(outpath +'/'+ df['filename'].iloc[0]+'_'+str(df['compound'].min())+'_'+str(df['compound'].max())+'.isox')\n",
    "        \n",
    "        print(name)\n",
    "        df.to_csv(name, sep='\\t',float_format='%.3f')\n",
    "    print('____________________________________')\n",
    "    print('New files are saved in ./' + outpath +'/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    #path=glob.glob(os.path.join('../data/',\"*.isox\"))\n",
    "    paths=glob.glob(os.path.join(\"94*.isox\"))\n",
    "\n",
    "    # Set split times for the new files\n",
    "    df1_split_times=[0,120]\n",
    "    df2_split_times=[120.05,210]\n",
    "    df3_split_times=[210.05,300]\n",
    "    df4_split_times=[300.05,330]\n",
    "\n",
    "    # these lines runs the function\n",
    "    for path in paths:\n",
    "        # do not process if folder exists \n",
    "        fold_name=path.split('.')[0]\n",
    "        isExist=os.path.exists('./'+fold_name)\n",
    "        if not isExist:\n",
    "            print(f'Folder \\'{fold_name}\\' does not exist -> start processing')\n",
    "            #preprocessing\n",
    "            dfs_list = preproc_isox(path, df1_split_times,df2_split_times,df3_split_times,df4_split_times)\n",
    "            print('____________________________________')\n",
    "            print('Current file is ' + path)\n",
    "\n",
    "            #if del #put a condition depending on the input\n",
    "            find_prob_comp(dfs_list)\n",
    "            #delete unpaired scans with unpaired isotopologs\n",
    "            dfs_list = delete_unpaired(dfs_list)\n",
    "\n",
    "            #save files\n",
    "            save(dfs_list)\n",
    "        else:\n",
    "            print(f'Folder \\'{fold_name}\\' already exists')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400' does not exist -> start processing\n",
      "The total length of the file is 0.011 min to 329.999 min\n",
      "____________________________________\n",
      "The chosed split of the file is into 4 files \n",
      " 1st [0, 120] min \n",
      " 2nd [120.05, 210] min \n",
      " 3rd [210.05, 300] min \n",
      " 4th [300.05, 330] min\n",
      "____________________________________\n",
      "Current file is 94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400.isox\n",
      "___Working on file #1\n",
      "___Working on file #2\n",
      "Problematic compound: 199; No of scans/compounds: 5561 / 5555.0\n",
      "   missing 0.11 % of the data\n",
      "Problematic compound: 261; No of scans/compounds: 5561 / 5560.0\n",
      "   missing 0.02 % of the data\n",
      "Problematic compound: 271; No of scans/compounds: 5561 / 5556.0\n",
      "   missing 0.09 % of the data\n",
      "___Working on file #3\n",
      "Problematic compound: 301; No of scans/compounds: 3163 / 3161.0\n",
      "   missing 0.06 % of the data\n",
      "Problematic compound: 313; No of scans/compounds: 3163 / 3159.5\n",
      "   missing 0.11 % of the data\n",
      "Problematic compound: 325; No of scans/compounds: 3163 / 3160.0\n",
      "   missing 0.09 % of the data\n",
      "Problematic compound: 349; No of scans/compounds: 3163 / 3079.0\n",
      "   missing 2.66 % of the data\n",
      "Problematic compound: 351; No of scans/compounds: 3163 / 3091.5\n",
      "   missing 2.26 % of the data\n",
      "Problematic compound: 353; No of scans/compounds: 3163 / 3161.5\n",
      "   missing 0.05 % of the data\n",
      "Problematic compound: 339; No of scans/compounds: 3163 / 2274.0\n",
      "   missing 28.11 % of the data\n",
      "___Working on file #4\n",
      "Problematic compound: 149; No of scans/compounds: 2076 / 1038.0\n",
      "   missing 50.00 % of the data\n",
      "Problematic compound: 163; No of scans/compounds: 2076 / 2075.0\n",
      "   missing 0.05 % of the data\n",
      "Problematic compound: 165; No of scans/compounds: 2076 / 1312.5\n",
      "   missing 36.78 % of the data\n",
      "Problematic compound: 191; No of scans/compounds: 2076 / 1615.0\n",
      "   missing 22.21 % of the data\n",
      "Problematic compound: 193; No of scans/compounds: 2076 / 2055.5\n",
      "   missing 0.99 % of the data\n",
      "Problematic compound: 199; No of scans/compounds: 2076 / 1466.0\n",
      "   missing 29.38 % of the data\n",
      "Problematic compound: 205; No of scans/compounds: 2076 / 2043.0\n",
      "   missing 1.59 % of the data\n",
      "Problematic compound: 207; No of scans/compounds: 2076 / 1843.5\n",
      "   missing 11.20 % of the data\n",
      "Problematic compound: 217; No of scans/compounds: 2076 / 2075.5\n",
      "   missing 0.02 % of the data\n",
      "Problematic compound: 219; No of scans/compounds: 2076 / 1825.0\n",
      "   missing 12.09 % of the data\n",
      "Problematic compound: 231; No of scans/compounds: 2076 / 2027.5\n",
      "   missing 2.34 % of the data\n",
      "Problematic compound: 245; No of scans/compounds: 2076 / 1827.5\n",
      "   missing 11.97 % of the data\n",
      "Problematic compound: 247; No of scans/compounds: 2076 / 2058.0\n",
      "   missing 0.87 % of the data\n",
      "Problematic compound: 261; No of scans/compounds: 2076 / 1097.5\n",
      "   missing 47.13 % of the data\n",
      "Problematic compound: 271; No of scans/compounds: 2076 / 782.5\n",
      "   missing 62.31 % of the data\n",
      "Problematic compound: 272; No of scans/compounds: 2076 / 2064.0\n",
      "   missing 0.58 % of the data\n",
      "Problematic compound: 285; No of scans/compounds: 2076 / 1060.0\n",
      "   missing 48.94 % of the data\n",
      "Problematic compound: 299; No of scans/compounds: 2076 / 1117.5\n",
      "   missing 46.17 % of the data\n",
      "Problematic compound: 301; No of scans/compounds: 2076 / 1041.5\n",
      "   missing 49.83 % of the data\n",
      "Problematic compound: 313; No of scans/compounds: 2076 / 1631.5\n",
      "   missing 21.41 % of the data\n",
      "Problematic compound: 327; No of scans/compounds: 2076 / 1999.0\n",
      "   missing 3.71 % of the data\n",
      "Problematic compound: 339; No of scans/compounds: 2076 / 1621.0\n",
      "   missing 21.92 % of the data\n",
      "Problematic compound: 349; No of scans/compounds: 2076 / 1625.5\n",
      "   missing 21.70 % of the data\n",
      "Problematic compound: 351; No of scans/compounds: 2076 / 1044.0\n",
      "   missing 49.71 % of the data\n",
      "Problematic compound: 353; No of scans/compounds: 2076 / 530.0\n",
      "   missing 74.47 % of the data\n",
      "Problematic compound: 325; No of scans/compounds: 2076 / 17.0\n",
      "   missing 99.18 % of the data\n",
      "Starting to delete unpaired lines\n",
      "Unpaired lines are deleted\n",
      "Unpaired lines are deleted\n",
      "Unpaired lines are deleted\n",
      "Unpaired lines are deleted\n",
      "____________________________________\n",
      "Currently saving:\n",
      "94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400/94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400_133_175.isox\n",
      "94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400/94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400_187_287.isox\n",
      "94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400/94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400_299_353.isox\n",
      "94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400/94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400_159_385.isox\n",
      "____________________________________\n",
      "New files are saved in ./94_Cholesterol_Nice-SBL1-85_vs_Nice-Comm-84_125-400/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_isox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
